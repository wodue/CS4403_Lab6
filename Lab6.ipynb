{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Task\n",
    "You will find a simple neural network implementation for modeling house prices, given certain features, here:\n",
    "\n",
    "https://colab.research.google.com/drive/1927LqJvRQQJs8qgg2ZupivhbWhPgIhTU?usp=sharing\n",
    "\n",
    "It uses the same dataset we used before:\n",
    "\n",
    "https://drive.google.com/file/d/1AYIPImmjBnUk5IHUiz1YYUhETVxelHlr/view?usp=share_link\n",
    "\n",
    "\n",
    "As you can see I am not a very good data scientist, and am getting a pretty abysmal MAE (mean absolute error) score.\n",
    "\n",
    "It is your task to explore the documentation for the Keras neural network library,  and adjust the configuration of the model to improve its performance.\n",
    "\n",
    "# Consider:\n",
    "\n",
    "     - using different combinations of features\n",
    "\n",
    "     - using more or less neurons\n",
    "\n",
    "     - adding more layers\n",
    "\n",
    "     - using a different activation function\n",
    "\n",
    "     - running more epochs\n",
    "\n",
    "     - anything else you note in the documentation that may help\n",
    "\n",
    "     - How low can you get the MAE score?\n",
    "\n",
    "\n",
    "ChatGPT says the following may be helpful to you:\n",
    "\n",
    "\n",
    "The TensorFlow Keras documentation provides detailed information on how to use Keras, which is a high-level neural networks API for TensorFlow. It includes documentation for the Keras API, as well as tutorials and examples for common use cases. Here are some resources to help you get started:\n",
    "\n",
    "The official TensorFlow Keras documentation: https://www.tensorflow.org/guide/keras This documentation provides a comprehensive guide to the Keras API, including information on how to use Keras with TensorFlow.\n",
    "\n",
    "Keras API reference: https://www.tensorflow.org/api_docs/python/tf/keras This provides an overview of the Keras API, including modules, classes, and functions.\n",
    "\n",
    "Keras tutorials: https://www.tensorflow.org/tutorials/keras These tutorials provide step-by-step guides for building various types of neural networks using Keras.\n",
    "\n",
    "Keras examples: https://www.tensorflow.org/examples?hl=en This provides a collection of Keras examples that cover a wide range of use cases, including computer vision, natural language processing, and more.\n",
    "\n",
    "Keras Getting Started Guide: https://www.tensorflow.org/guide/keras/overview This guide provides an overview of the Keras API and helps you get started with building your first neural network using Keras.\n",
    "\n",
    "Keras FAQs: https://www.tensorflow.org/guide/keras/faq This provides answers to frequently asked questions about Keras, including troubleshooting tips and advice on best practices.\n",
    "\n",
    "UPDATE: some of the above links are no longer functional (recall that GPT uses data from 2021).  Apologies!\n",
    "\n",
    "Links to updated resources are below:\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/sequential_model \n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "\n",
    "https://www.tensorflow.org/tutorials\n",
    "\n",
    "Hand in\n",
    "Your Jupyter Notebook showing what you tried, together with output. \n",
    "\n",
    "Boldly display your lowest MAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date     price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000  221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000  538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000  180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000  604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000  510000.0         3       2.00   \n",
       "...           ...              ...       ...       ...        ...   \n",
       "21608   263000018  20140521T000000  360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000  400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000  402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000  400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000  325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "0             1180      5650     1.0           0     0  ...      7   \n",
       "1             2570      7242     2.0           0     0  ...      7   \n",
       "2              770     10000     1.0           0     0  ...      6   \n",
       "3             1960      5000     1.0           0     0  ...      7   \n",
       "4             1680      8080     1.0           0     0  ...      8   \n",
       "...            ...       ...     ...         ...   ...  ...    ...   \n",
       "21608         1530      1131     3.0           0     0  ...      8   \n",
       "21609         2310      5813     2.0           0     0  ...      8   \n",
       "21610         1020      1350     2.0           0     0  ...      7   \n",
       "21611         1600      2388     2.0           0     0  ...      8   \n",
       "21612         1020      1076     2.0           0     0  ...      7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0          1180.0              0      1955             0    98178  47.5112   \n",
       "1          2170.0            400      1951          1991    98125  47.7210   \n",
       "2           770.0              0      1933             0    98028  47.7379   \n",
       "3          1050.0            910      1965             0    98136  47.5208   \n",
       "4          1680.0              0      1987             0    98074  47.6168   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21608      1530.0              0      2009             0    98103  47.6993   \n",
       "21609      2310.0              0      2014             0    98146  47.5107   \n",
       "21610      1020.0              0      2009             0    98144  47.5944   \n",
       "21611      1600.0              0      2004             0    98027  47.5345   \n",
       "21612      1020.0              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "...        ...            ...         ...  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset into code\n",
    "df = pd.read_csv('house_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
       "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
       "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
       "       'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show our columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wodue\\AppData\\Local\\Temp\\ipykernel_1210920\\3245462674.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df = (df - df.mean()) / df.std()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>condition</th>\n",
       "      <th>date</th>\n",
       "      <th>floors</th>\n",
       "      <th>grade</th>\n",
       "      <th>id</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>price</th>\n",
       "      <th>...</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>view</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.447430</td>\n",
       "      <td>-0.398728</td>\n",
       "      <td>-0.629172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.915406</td>\n",
       "      <td>-0.558823</td>\n",
       "      <td>0.886126</td>\n",
       "      <td>-0.352564</td>\n",
       "      <td>-0.306072</td>\n",
       "      <td>-0.866697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658666</td>\n",
       "      <td>-0.979812</td>\n",
       "      <td>-0.943333</td>\n",
       "      <td>-0.228316</td>\n",
       "      <td>-0.260709</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>-0.544885</td>\n",
       "      <td>-0.210124</td>\n",
       "      <td>1.870108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.175603</td>\n",
       "      <td>-0.398728</td>\n",
       "      <td>-0.629172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>-0.558823</td>\n",
       "      <td>0.637496</td>\n",
       "      <td>1.161541</td>\n",
       "      <td>-0.746324</td>\n",
       "      <td>-0.005688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245136</td>\n",
       "      <td>0.533622</td>\n",
       "      <td>-0.432676</td>\n",
       "      <td>-0.189881</td>\n",
       "      <td>-0.187863</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>-0.681063</td>\n",
       "      <td>4.746568</td>\n",
       "      <td>0.879547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.447430</td>\n",
       "      <td>-1.473925</td>\n",
       "      <td>-0.629172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.915406</td>\n",
       "      <td>-1.409554</td>\n",
       "      <td>0.365435</td>\n",
       "      <td>1.283507</td>\n",
       "      <td>-0.135652</td>\n",
       "      <td>-0.980827</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658666</td>\n",
       "      <td>-1.426221</td>\n",
       "      <td>1.070115</td>\n",
       "      <td>-0.123296</td>\n",
       "      <td>-0.172371</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>-1.293862</td>\n",
       "      <td>-0.210124</td>\n",
       "      <td>-0.933367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.149422</td>\n",
       "      <td>0.676469</td>\n",
       "      <td>2.444237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.915406</td>\n",
       "      <td>-0.558823</td>\n",
       "      <td>-0.727639</td>\n",
       "      <td>-0.283281</td>\n",
       "      <td>-1.271787</td>\n",
       "      <td>0.174086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.397483</td>\n",
       "      <td>-0.130547</td>\n",
       "      <td>-0.914153</td>\n",
       "      <td>-0.244009</td>\n",
       "      <td>-0.284515</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>-0.204441</td>\n",
       "      <td>-0.210124</td>\n",
       "      <td>1.085135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.149004</td>\n",
       "      <td>-0.398728</td>\n",
       "      <td>-0.629172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.915406</td>\n",
       "      <td>0.291909</td>\n",
       "      <td>-0.912860</td>\n",
       "      <td>0.409541</td>\n",
       "      <td>1.199307</td>\n",
       "      <td>-0.081956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658666</td>\n",
       "      <td>-0.435412</td>\n",
       "      <td>-0.272184</td>\n",
       "      <td>-0.169649</td>\n",
       "      <td>-0.192844</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>0.544535</td>\n",
       "      <td>-0.210124</td>\n",
       "      <td>-0.073634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>0.500209</td>\n",
       "      <td>-0.398728</td>\n",
       "      <td>-0.629172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.788374</td>\n",
       "      <td>0.291909</td>\n",
       "      <td>-1.500853</td>\n",
       "      <td>1.004935</td>\n",
       "      <td>-0.938047</td>\n",
       "      <td>-0.490533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658666</td>\n",
       "      <td>-0.598732</td>\n",
       "      <td>-0.666119</td>\n",
       "      <td>-0.337417</td>\n",
       "      <td>-0.412371</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>1.293512</td>\n",
       "      <td>-0.210124</td>\n",
       "      <td>0.468371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>0.500209</td>\n",
       "      <td>0.676469</td>\n",
       "      <td>-0.629172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>0.291909</td>\n",
       "      <td>0.702142</td>\n",
       "      <td>-0.356172</td>\n",
       "      <td>-1.051660</td>\n",
       "      <td>-0.381579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658666</td>\n",
       "      <td>0.250534</td>\n",
       "      <td>-0.228413</td>\n",
       "      <td>-0.224381</td>\n",
       "      <td>-0.203942</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>1.463734</td>\n",
       "      <td>-0.210124</td>\n",
       "      <td>1.272034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>-1.772037</td>\n",
       "      <td>-1.473925</td>\n",
       "      <td>-0.629172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>-0.558823</td>\n",
       "      <td>-1.062726</td>\n",
       "      <td>0.247882</td>\n",
       "      <td>-0.604307</td>\n",
       "      <td>-0.375856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658666</td>\n",
       "      <td>-1.154021</td>\n",
       "      <td>-1.410220</td>\n",
       "      <td>-0.332129</td>\n",
       "      <td>-0.394132</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>1.293512</td>\n",
       "      <td>-0.210124</td>\n",
       "      <td>1.234654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>0.500209</td>\n",
       "      <td>-0.398728</td>\n",
       "      <td>-0.629172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>0.291909</td>\n",
       "      <td>-1.491011</td>\n",
       "      <td>-0.184410</td>\n",
       "      <td>1.028887</td>\n",
       "      <td>-0.381579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658666</td>\n",
       "      <td>-0.522516</td>\n",
       "      <td>-0.841202</td>\n",
       "      <td>-0.307069</td>\n",
       "      <td>-0.420502</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>1.123290</td>\n",
       "      <td>-0.210124</td>\n",
       "      <td>-0.952056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>-1.772037</td>\n",
       "      <td>-1.473925</td>\n",
       "      <td>-0.629172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>-0.558823</td>\n",
       "      <td>-1.062726</td>\n",
       "      <td>0.245717</td>\n",
       "      <td>-0.604307</td>\n",
       "      <td>-0.585868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658666</td>\n",
       "      <td>-1.154021</td>\n",
       "      <td>-1.410220</td>\n",
       "      <td>-0.338744</td>\n",
       "      <td>-0.417938</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.087171</td>\n",
       "      <td>1.259468</td>\n",
       "      <td>-0.210124</td>\n",
       "      <td>1.234654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bathrooms  bedrooms  condition date    floors     grade        id  \\\n",
       "0      -1.447430 -0.398728  -0.629172  NaN -0.915406 -0.558823  0.886126   \n",
       "1       0.175603 -0.398728  -0.629172  NaN  0.936484 -0.558823  0.637496   \n",
       "2      -1.447430 -1.473925  -0.629172  NaN -0.915406 -1.409554  0.365435   \n",
       "3       1.149422  0.676469   2.444237  NaN -0.915406 -0.558823 -0.727639   \n",
       "4      -0.149004 -0.398728  -0.629172  NaN -0.915406  0.291909 -0.912860   \n",
       "...          ...       ...        ...  ...       ...       ...       ...   \n",
       "21608   0.500209 -0.398728  -0.629172  NaN  2.788374  0.291909 -1.500853   \n",
       "21609   0.500209  0.676469  -0.629172  NaN  0.936484  0.291909  0.702142   \n",
       "21610  -1.772037 -1.473925  -0.629172  NaN  0.936484 -0.558823 -1.062726   \n",
       "21611   0.500209 -0.398728  -0.629172  NaN  0.936484  0.291909 -1.491011   \n",
       "21612  -1.772037 -1.473925  -0.629172  NaN  0.936484 -0.558823 -1.062726   \n",
       "\n",
       "            lat      long     price  ...  sqft_basement  sqft_living  \\\n",
       "0     -0.352564 -0.306072 -0.866697  ...      -0.658666    -0.979812   \n",
       "1      1.161541 -0.746324 -0.005688  ...       0.245136     0.533622   \n",
       "2      1.283507 -0.135652 -0.980827  ...      -0.658666    -1.426221   \n",
       "3     -0.283281 -1.271787  0.174086  ...       1.397483    -0.130547   \n",
       "4      0.409541  1.199307 -0.081956  ...      -0.658666    -0.435412   \n",
       "...         ...       ...       ...  ...            ...          ...   \n",
       "21608  1.004935 -0.938047 -0.490533  ...      -0.658666    -0.598732   \n",
       "21609 -0.356172 -1.051660 -0.381579  ...      -0.658666     0.250534   \n",
       "21610  0.247882 -0.604307 -0.375856  ...      -0.658666    -1.154021   \n",
       "21611 -0.184410  1.028887 -0.381579  ...      -0.658666    -0.522516   \n",
       "21612  0.245717 -0.604307 -0.585868  ...      -0.658666    -1.154021   \n",
       "\n",
       "       sqft_living15  sqft_lot  sqft_lot15      view  waterfront  yr_built  \\\n",
       "0          -0.943333 -0.228316   -0.260709 -0.305752   -0.087171 -0.544885   \n",
       "1          -0.432676 -0.189881   -0.187863 -0.305752   -0.087171 -0.681063   \n",
       "2           1.070115 -0.123296   -0.172371 -0.305752   -0.087171 -1.293862   \n",
       "3          -0.914153 -0.244009   -0.284515 -0.305752   -0.087171 -0.204441   \n",
       "4          -0.272184 -0.169649   -0.192844 -0.305752   -0.087171  0.544535   \n",
       "...              ...       ...         ...       ...         ...       ...   \n",
       "21608      -0.666119 -0.337417   -0.412371 -0.305752   -0.087171  1.293512   \n",
       "21609      -0.228413 -0.224381   -0.203942 -0.305752   -0.087171  1.463734   \n",
       "21610      -1.410220 -0.332129   -0.394132 -0.305752   -0.087171  1.293512   \n",
       "21611      -0.841202 -0.307069   -0.420502 -0.305752   -0.087171  1.123290   \n",
       "21612      -1.410220 -0.338744   -0.417938 -0.305752   -0.087171  1.259468   \n",
       "\n",
       "       yr_renovated   zipcode  \n",
       "0         -0.210124  1.870108  \n",
       "1          4.746568  0.879547  \n",
       "2         -0.210124 -0.933367  \n",
       "3         -0.210124  1.085135  \n",
       "4         -0.210124 -0.073634  \n",
       "...             ...       ...  \n",
       "21608     -0.210124  0.468371  \n",
       "21609     -0.210124  1.272034  \n",
       "21610     -0.210124  1.234654  \n",
       "21611     -0.210124 -0.952056  \n",
       "21612     -0.210124  1.234654  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize our data\n",
    "df = (df - df.mean()) / df.std()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for the feature combinations function\n",
    "import itertools\n",
    "\n",
    "# Tensorflows api had two very elegant functions to\n",
    "# define an uncompiled neural network and a compiled neural network\n",
    "# then I added the elegant code given by Professor van der Laan for making a sequential model\n",
    "def get_uncompiled_model(inputShape, activationFunction, neuronList):\n",
    "    # define a neural network with one hidden layer\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # study these hyperparameters and experiment!\n",
    "    # try out different activation functions in particular\n",
    "    for i in range(len(neuronList)):\n",
    "        if i == 0:\n",
    "            model.add(layers.Dense(neuronList[i], activation=activationFunction, input_shape=inputShape))\n",
    "        else:\n",
    "            model.add(layers.Dense(neuronList[i], activation=activationFunction))\n",
    "\n",
    "    # output layer\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_compiled_model(inputShape, activationFunction='relu', neuronList= [64, 32, 16], optimFuction='rmsprop', lossFunction='mse', metricsFunction='mae'):\n",
    "    model = get_uncompiled_model(inputShape, activationFunction, neuronList)\n",
    "    model.compile(\n",
    "        optimizer=optimFuction,\n",
    "        loss=lossFunction,\n",
    "        metrics=[metricsFunction]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define a function that takes in a dataFrame\n",
    "# and returns a list of all the possible feature combinations\n",
    "def featureCombinations(dataFrame):\n",
    "    '''\n",
    "    Takes in a dataFrame and returns a list of lists of all the possible feature combinations\n",
    "    that you want to run against the column to be predicted\n",
    "    '''\n",
    "\n",
    "    combinations = []\n",
    "\n",
    "    columnNames = dataFrame.columns\n",
    "\n",
    "    for i in range(len(columnNames)):\n",
    "        combinations.extend(list(itertools.combinations(columnNames, i)))\n",
    "\n",
    "    uniqueCombinations = list(set(combinations))\n",
    "\n",
    "    for i in range(len(uniqueCombinations) - 1):\n",
    "        if uniqueCombinations[i] == ():\n",
    "            # Remove the empty set out of the list\n",
    "            uniqueCombinations.pop(i)\n",
    "\n",
    "    # For debugging purposes\n",
    "    # print(uniqueCombinations)\n",
    "\n",
    "    return uniqueCombinations\n",
    "\n",
    "# Define a function that will take a list of features, a target feature and a neural network\n",
    "# and that returns the mean absolute error of the neural network\n",
    "def evalNN(model, features, target, epochNum=100):\n",
    "    '''\n",
    "    Takes in a neural network, a list of features and a target feature\n",
    "    and returns the mean absolute error and loss\n",
    "    of the test of the neural network\n",
    "    '''\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    trainFeatures, testFeatures, trainTarget, testTarget = train_test_split(features, target, test_size=0.2)\n",
    "\n",
    "    # Train the neural network\n",
    "    history = model.fit(trainFeatures, trainTarget, epochs=epochNum, verbose=1)\n",
    "\n",
    "    test_loss, test_mae = model.evaluate(testFeatures, testTarget, verbose=1)\n",
    "\n",
    "    return history, test_loss, test_mae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5576 - mae: 0.4805\n",
      "Epoch 2/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5179 - mae: 0.4674\n",
      "Epoch 3/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5099 - mae: 0.4642\n",
      "Epoch 4/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5017 - mae: 0.4617\n",
      "Epoch 5/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4953 - mae: 0.4601\n",
      "Epoch 6/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4943 - mae: 0.4589\n",
      "Epoch 7/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4898 - mae: 0.4570\n",
      "Epoch 8/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4856 - mae: 0.4570\n",
      "Epoch 9/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4829 - mae: 0.4564\n",
      "Epoch 10/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4791 - mae: 0.4544\n",
      "Epoch 11/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4769 - mae: 0.4543\n",
      "Epoch 12/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4740 - mae: 0.4524\n",
      "Epoch 13/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4738 - mae: 0.4522\n",
      "Epoch 14/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4719 - mae: 0.4520\n",
      "Epoch 15/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4658 - mae: 0.4513\n",
      "Epoch 16/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4682 - mae: 0.4494\n",
      "Epoch 17/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4688 - mae: 0.4497\n",
      "Epoch 18/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4655 - mae: 0.4491\n",
      "Epoch 19/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4608 - mae: 0.4484\n",
      "Epoch 20/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4605 - mae: 0.4480\n",
      "Epoch 21/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4617 - mae: 0.4473\n",
      "Epoch 22/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4607 - mae: 0.4483\n",
      "Epoch 23/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4580 - mae: 0.4464\n",
      "Epoch 24/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4564 - mae: 0.4474\n",
      "Epoch 25/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4555 - mae: 0.4455\n",
      "Epoch 26/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4557 - mae: 0.4464\n",
      "Epoch 27/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4515 - mae: 0.4449\n",
      "Epoch 28/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4528 - mae: 0.4454\n",
      "Epoch 29/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4532 - mae: 0.4445\n",
      "Epoch 30/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4552 - mae: 0.4447\n",
      "Epoch 31/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4505 - mae: 0.4446\n",
      "Epoch 32/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4495 - mae: 0.4442\n",
      "Epoch 33/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4507 - mae: 0.4443\n",
      "Epoch 34/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4469 - mae: 0.4431\n",
      "Epoch 35/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4456 - mae: 0.4424\n",
      "Epoch 36/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4461 - mae: 0.4431\n",
      "Epoch 37/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4449 - mae: 0.4436\n",
      "Epoch 38/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4432 - mae: 0.4411\n",
      "Epoch 39/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4468 - mae: 0.4420\n",
      "Epoch 40/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4435 - mae: 0.4416\n",
      "Epoch 41/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4431 - mae: 0.4407\n",
      "Epoch 42/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4438 - mae: 0.4419\n",
      "Epoch 43/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4448 - mae: 0.4416\n",
      "Epoch 44/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4407 - mae: 0.4398\n",
      "Epoch 45/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4448 - mae: 0.4411\n",
      "Epoch 46/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4406 - mae: 0.4411\n",
      "Epoch 47/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4408 - mae: 0.4400\n",
      "Epoch 48/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4420 - mae: 0.4407\n",
      "Epoch 49/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4421 - mae: 0.4405\n",
      "Epoch 50/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4420 - mae: 0.4406\n",
      "Epoch 51/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4389 - mae: 0.4395\n",
      "Epoch 52/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4381 - mae: 0.4388\n",
      "Epoch 53/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4384 - mae: 0.4394\n",
      "Epoch 54/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4379 - mae: 0.4390\n",
      "Epoch 55/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4386 - mae: 0.4407\n",
      "Epoch 56/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4376 - mae: 0.4393\n",
      "Epoch 57/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4405 - mae: 0.4401\n",
      "Epoch 58/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4350 - mae: 0.4381\n",
      "Epoch 59/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4367 - mae: 0.4396\n",
      "Epoch 60/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4351 - mae: 0.4390\n",
      "Epoch 61/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4360 - mae: 0.4383\n",
      "Epoch 62/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4393 - mae: 0.4392\n",
      "Epoch 63/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4360 - mae: 0.4383\n",
      "Epoch 64/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4374 - mae: 0.4386\n",
      "Epoch 65/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4358 - mae: 0.4379\n",
      "Epoch 66/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4380 - mae: 0.4382\n",
      "Epoch 67/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4297 - mae: 0.4379\n",
      "Epoch 68/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4318 - mae: 0.4371\n",
      "Epoch 69/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4345 - mae: 0.4370\n",
      "Epoch 70/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4311 - mae: 0.4370\n",
      "Epoch 71/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4326 - mae: 0.4376\n",
      "Epoch 72/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4312 - mae: 0.4370\n",
      "Epoch 73/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4289 - mae: 0.4373\n",
      "Epoch 74/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4283 - mae: 0.4365\n",
      "Epoch 75/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4258 - mae: 0.4369\n",
      "Epoch 76/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4266 - mae: 0.4357\n",
      "Epoch 77/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4292 - mae: 0.4358\n",
      "Epoch 78/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4278 - mae: 0.4352\n",
      "Epoch 79/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4252 - mae: 0.4352\n",
      "Epoch 80/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4292 - mae: 0.4362\n",
      "Epoch 81/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4296 - mae: 0.4342\n",
      "Epoch 82/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4295 - mae: 0.4358\n",
      "Epoch 83/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4304 - mae: 0.4362\n",
      "Epoch 84/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4272 - mae: 0.4354\n",
      "Epoch 85/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4275 - mae: 0.4362\n",
      "Epoch 86/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4230 - mae: 0.4343\n",
      "Epoch 87/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4272 - mae: 0.4358\n",
      "Epoch 88/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4279 - mae: 0.4360\n",
      "Epoch 89/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4253 - mae: 0.4362\n",
      "Epoch 90/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4256 - mae: 0.4346\n",
      "Epoch 91/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4260 - mae: 0.4352\n",
      "Epoch 92/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4263 - mae: 0.4355\n",
      "Epoch 93/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4216 - mae: 0.4353\n",
      "Epoch 94/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4217 - mae: 0.4346\n",
      "Epoch 95/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4241 - mae: 0.4342\n",
      "Epoch 96/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4239 - mae: 0.4344\n",
      "Epoch 97/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4252 - mae: 0.4346\n",
      "Epoch 98/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4218 - mae: 0.4344\n",
      "Epoch 99/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4222 - mae: 0.4355\n",
      "Epoch 100/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4204 - mae: 0.4338\n",
      "Epoch 101/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4202 - mae: 0.4332\n",
      "Epoch 102/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4226 - mae: 0.4345\n",
      "Epoch 103/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4222 - mae: 0.4336\n",
      "Epoch 104/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4217 - mae: 0.4344\n",
      "Epoch 105/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4230 - mae: 0.4333\n",
      "Epoch 106/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4197 - mae: 0.4336\n",
      "Epoch 107/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4188 - mae: 0.4336\n",
      "Epoch 108/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4173 - mae: 0.4329\n",
      "Epoch 109/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4174 - mae: 0.4340\n",
      "Epoch 110/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4197 - mae: 0.4335\n",
      "Epoch 111/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4146 - mae: 0.4338\n",
      "Epoch 112/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4204 - mae: 0.4325\n",
      "Epoch 113/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4179 - mae: 0.4339\n",
      "Epoch 114/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4154 - mae: 0.4331\n",
      "Epoch 115/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4153 - mae: 0.4322\n",
      "Epoch 116/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4137 - mae: 0.4343\n",
      "Epoch 117/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4122 - mae: 0.4330\n",
      "Epoch 118/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4176 - mae: 0.4336\n",
      "Epoch 119/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4129 - mae: 0.4318\n",
      "Epoch 120/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4120 - mae: 0.4318\n",
      "Epoch 121/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4101 - mae: 0.4312\n",
      "Epoch 122/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4139 - mae: 0.4320\n",
      "Epoch 123/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4141 - mae: 0.4322\n",
      "Epoch 124/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4123 - mae: 0.4320\n",
      "Epoch 125/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4157 - mae: 0.4319\n",
      "Epoch 126/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4141 - mae: 0.4326\n",
      "Epoch 127/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4121 - mae: 0.4313\n",
      "Epoch 128/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4094 - mae: 0.4314\n",
      "Epoch 129/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4094 - mae: 0.4315\n",
      "Epoch 130/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4093 - mae: 0.4319\n",
      "Epoch 131/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4108 - mae: 0.4314\n",
      "Epoch 132/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4093 - mae: 0.4317\n",
      "Epoch 133/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4117 - mae: 0.4307\n",
      "Epoch 134/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4107 - mae: 0.4319\n",
      "Epoch 135/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4083 - mae: 0.4312\n",
      "Epoch 136/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4086 - mae: 0.4307\n",
      "Epoch 137/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4111 - mae: 0.4313\n",
      "Epoch 138/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4091 - mae: 0.4302\n",
      "Epoch 139/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4087 - mae: 0.4305\n",
      "Epoch 140/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4060 - mae: 0.4292\n",
      "Epoch 141/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4074 - mae: 0.4296\n",
      "Epoch 142/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4066 - mae: 0.4292\n",
      "Epoch 143/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4058 - mae: 0.4295\n",
      "Epoch 144/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4039 - mae: 0.4296\n",
      "Epoch 145/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4070 - mae: 0.4290\n",
      "Epoch 146/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4039 - mae: 0.4291\n",
      "Epoch 147/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4060 - mae: 0.4299\n",
      "Epoch 148/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4036 - mae: 0.4284\n",
      "Epoch 149/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4046 - mae: 0.4288\n",
      "Epoch 150/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4054 - mae: 0.4293\n",
      "Epoch 151/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4067 - mae: 0.4296\n",
      "Epoch 152/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4016 - mae: 0.4272\n",
      "Epoch 153/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4057 - mae: 0.4300\n",
      "Epoch 154/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.4028 - mae: 0.4290\n",
      "Epoch 155/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4030 - mae: 0.4298\n",
      "Epoch 156/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4031 - mae: 0.4292\n",
      "Epoch 157/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4028 - mae: 0.4281\n",
      "Epoch 158/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4038 - mae: 0.4289\n",
      "Epoch 159/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4008 - mae: 0.4285\n",
      "Epoch 160/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4008 - mae: 0.4294\n",
      "Epoch 161/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4016 - mae: 0.4277\n",
      "Epoch 162/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3965 - mae: 0.4272\n",
      "Epoch 163/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4003 - mae: 0.4293\n",
      "Epoch 164/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4015 - mae: 0.4282\n",
      "Epoch 165/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4006 - mae: 0.4266\n",
      "Epoch 166/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4016 - mae: 0.4272\n",
      "Epoch 167/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3993 - mae: 0.4282\n",
      "Epoch 168/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4016 - mae: 0.4279\n",
      "Epoch 169/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3976 - mae: 0.4277\n",
      "Epoch 170/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3979 - mae: 0.4283\n",
      "Epoch 171/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3987 - mae: 0.4287\n",
      "Epoch 172/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3991 - mae: 0.4273\n",
      "Epoch 173/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.3976 - mae: 0.4268\n",
      "Epoch 174/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.3992 - mae: 0.4265\n",
      "Epoch 175/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3989 - mae: 0.4273\n",
      "Epoch 176/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.3950 - mae: 0.4278\n",
      "Epoch 177/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3932 - mae: 0.4268\n",
      "Epoch 178/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3975 - mae: 0.4263\n",
      "Epoch 179/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3943 - mae: 0.4268\n",
      "Epoch 180/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3945 - mae: 0.4274\n",
      "Epoch 181/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3920 - mae: 0.4250\n",
      "Epoch 182/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3945 - mae: 0.4259\n",
      "Epoch 183/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3978 - mae: 0.4270\n",
      "Epoch 184/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3934 - mae: 0.4266\n",
      "Epoch 185/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3949 - mae: 0.4264\n",
      "Epoch 186/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3891 - mae: 0.4255\n",
      "Epoch 187/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3921 - mae: 0.4267\n",
      "Epoch 188/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3941 - mae: 0.4268\n",
      "Epoch 189/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.3931 - mae: 0.4255\n",
      "Epoch 190/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3938 - mae: 0.4256\n",
      "Epoch 191/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3930 - mae: 0.4256\n",
      "Epoch 192/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3921 - mae: 0.4243\n",
      "Epoch 193/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3908 - mae: 0.4251\n",
      "Epoch 194/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3929 - mae: 0.4251\n",
      "Epoch 195/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3928 - mae: 0.4258\n",
      "Epoch 196/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3876 - mae: 0.4241\n",
      "Epoch 197/200\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.3898 - mae: 0.4252\n",
      "Epoch 198/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3913 - mae: 0.4253\n",
      "Epoch 199/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3920 - mae: 0.4250\n",
      "Epoch 200/200\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3936 - mae: 0.4242\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.5728 - mae: 0.4736\n",
      "Epoch 1/100\n",
      "541/541 [==============================] - 2s 2ms/step - loss: 0.9943 - mae: 0.6278\n",
      "Epoch 2/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.8532 - mae: 0.5651\n",
      "Epoch 3/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.7060 - mae: 0.5104\n",
      "Epoch 4/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.6525 - mae: 0.4961\n",
      "Epoch 5/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.6183 - mae: 0.4869\n",
      "Epoch 6/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5966 - mae: 0.4820\n",
      "Epoch 7/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5818 - mae: 0.4778\n",
      "Epoch 8/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5716 - mae: 0.4755\n",
      "Epoch 9/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5637 - mae: 0.4726\n",
      "Epoch 10/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5575 - mae: 0.4718\n",
      "Epoch 11/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5510 - mae: 0.4694\n",
      "Epoch 12/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5455 - mae: 0.4685\n",
      "Epoch 13/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5402 - mae: 0.4660\n",
      "Epoch 14/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5371 - mae: 0.4664\n",
      "Epoch 15/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5326 - mae: 0.4649\n",
      "Epoch 16/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5287 - mae: 0.4637\n",
      "Epoch 17/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5259 - mae: 0.4629\n",
      "Epoch 18/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5221 - mae: 0.4625\n",
      "Epoch 19/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5194 - mae: 0.4608\n",
      "Epoch 20/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5182 - mae: 0.4619\n",
      "Epoch 21/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5145 - mae: 0.4603\n",
      "Epoch 22/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5120 - mae: 0.4593\n",
      "Epoch 23/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5107 - mae: 0.4592\n",
      "Epoch 24/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5089 - mae: 0.4591\n",
      "Epoch 25/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5077 - mae: 0.4583\n",
      "Epoch 26/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5059 - mae: 0.4581\n",
      "Epoch 27/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5031 - mae: 0.4569\n",
      "Epoch 28/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5021 - mae: 0.4570\n",
      "Epoch 29/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5008 - mae: 0.4570\n",
      "Epoch 30/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4998 - mae: 0.4560\n",
      "Epoch 31/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4984 - mae: 0.4559\n",
      "Epoch 32/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4968 - mae: 0.4553\n",
      "Epoch 33/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4951 - mae: 0.4553\n",
      "Epoch 34/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4934 - mae: 0.4551\n",
      "Epoch 35/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4919 - mae: 0.4542\n",
      "Epoch 36/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4901 - mae: 0.4537\n",
      "Epoch 37/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4893 - mae: 0.4531\n",
      "Epoch 38/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4883 - mae: 0.4530\n",
      "Epoch 39/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4871 - mae: 0.4530\n",
      "Epoch 40/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4860 - mae: 0.4529\n",
      "Epoch 41/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4848 - mae: 0.4519\n",
      "Epoch 42/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4837 - mae: 0.4518\n",
      "Epoch 43/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4826 - mae: 0.4518\n",
      "Epoch 44/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4817 - mae: 0.4512\n",
      "Epoch 45/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4802 - mae: 0.4506\n",
      "Epoch 46/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4797 - mae: 0.4510\n",
      "Epoch 47/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4789 - mae: 0.4509\n",
      "Epoch 48/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4778 - mae: 0.4503\n",
      "Epoch 49/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4767 - mae: 0.4502\n",
      "Epoch 50/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4760 - mae: 0.4497\n",
      "Epoch 51/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4752 - mae: 0.4499\n",
      "Epoch 52/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4744 - mae: 0.4494\n",
      "Epoch 53/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4740 - mae: 0.4495\n",
      "Epoch 54/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4737 - mae: 0.4497\n",
      "Epoch 55/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4729 - mae: 0.4492\n",
      "Epoch 56/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4722 - mae: 0.4498\n",
      "Epoch 57/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4715 - mae: 0.4485\n",
      "Epoch 58/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4701 - mae: 0.4489\n",
      "Epoch 59/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4710 - mae: 0.4483\n",
      "Epoch 60/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4695 - mae: 0.4488\n",
      "Epoch 61/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4693 - mae: 0.4483\n",
      "Epoch 62/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4686 - mae: 0.4481\n",
      "Epoch 63/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4693 - mae: 0.4489\n",
      "Epoch 64/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4687 - mae: 0.4485\n",
      "Epoch 65/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4674 - mae: 0.4477\n",
      "Epoch 66/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4674 - mae: 0.4480\n",
      "Epoch 67/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4659 - mae: 0.4477\n",
      "Epoch 68/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4666 - mae: 0.4479\n",
      "Epoch 69/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4659 - mae: 0.4476\n",
      "Epoch 70/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4648 - mae: 0.4472\n",
      "Epoch 71/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4655 - mae: 0.4476\n",
      "Epoch 72/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4641 - mae: 0.4472\n",
      "Epoch 73/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4638 - mae: 0.4471\n",
      "Epoch 74/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4638 - mae: 0.4473\n",
      "Epoch 75/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4634 - mae: 0.4470\n",
      "Epoch 76/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4625 - mae: 0.4472\n",
      "Epoch 77/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4617 - mae: 0.4464\n",
      "Epoch 78/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4621 - mae: 0.4473\n",
      "Epoch 79/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4611 - mae: 0.4468\n",
      "Epoch 80/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4618 - mae: 0.4469\n",
      "Epoch 81/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4610 - mae: 0.4462\n",
      "Epoch 82/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4606 - mae: 0.4466\n",
      "Epoch 83/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4602 - mae: 0.4463\n",
      "Epoch 84/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4598 - mae: 0.4468\n",
      "Epoch 85/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4592 - mae: 0.4460\n",
      "Epoch 86/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4588 - mae: 0.4461\n",
      "Epoch 87/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4573 - mae: 0.4459\n",
      "Epoch 88/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4581 - mae: 0.4463\n",
      "Epoch 89/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4578 - mae: 0.4461\n",
      "Epoch 90/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4583 - mae: 0.4456\n",
      "Epoch 91/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4579 - mae: 0.4462\n",
      "Epoch 92/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4573 - mae: 0.4460\n",
      "Epoch 93/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4557 - mae: 0.4453\n",
      "Epoch 94/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4566 - mae: 0.4457\n",
      "Epoch 95/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4554 - mae: 0.4451\n",
      "Epoch 96/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4554 - mae: 0.4455\n",
      "Epoch 97/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4565 - mae: 0.4458\n",
      "Epoch 98/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4553 - mae: 0.4451\n",
      "Epoch 99/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4563 - mae: 0.4459\n",
      "Epoch 100/100\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.4547 - mae: 0.4456\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.4945 - mae: 0.4677\n",
      "Epoch 1/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.5731 - mae: 0.4514\n",
      "Epoch 2/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3843 - mae: 0.3814\n",
      "Epoch 3/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3665 - mae: 0.3806\n",
      "Epoch 4/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3564 - mae: 0.3805\n",
      "Epoch 5/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3510 - mae: 0.3788\n",
      "Epoch 6/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3459 - mae: 0.3768\n",
      "Epoch 7/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3424 - mae: 0.3753\n",
      "Epoch 8/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3380 - mae: 0.3748\n",
      "Epoch 9/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3358 - mae: 0.3724\n",
      "Epoch 10/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3355 - mae: 0.3722\n",
      "Epoch 11/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3314 - mae: 0.3716\n",
      "Epoch 12/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3308 - mae: 0.3703\n",
      "Epoch 13/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3309 - mae: 0.3709\n",
      "Epoch 14/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3283 - mae: 0.3704\n",
      "Epoch 15/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3270 - mae: 0.3704\n",
      "Epoch 16/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3284 - mae: 0.3703\n",
      "Epoch 17/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3254 - mae: 0.3689\n",
      "Epoch 18/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3224 - mae: 0.3690\n",
      "Epoch 19/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3220 - mae: 0.3690\n",
      "Epoch 20/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3217 - mae: 0.3693\n",
      "Epoch 21/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3194 - mae: 0.3684\n",
      "Epoch 22/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3193 - mae: 0.3680\n",
      "Epoch 23/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3194 - mae: 0.3679\n",
      "Epoch 24/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3177 - mae: 0.3674\n",
      "Epoch 25/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3173 - mae: 0.3676\n",
      "Epoch 26/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3161 - mae: 0.3675\n",
      "Epoch 27/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3162 - mae: 0.3661\n",
      "Epoch 28/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3184 - mae: 0.3678\n",
      "Epoch 29/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3154 - mae: 0.3668\n",
      "Epoch 30/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3154 - mae: 0.3670\n",
      "Epoch 31/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3156 - mae: 0.3664\n",
      "Epoch 32/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3141 - mae: 0.3655\n",
      "Epoch 33/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3151 - mae: 0.3658\n",
      "Epoch 34/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3145 - mae: 0.3657\n",
      "Epoch 35/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3117 - mae: 0.3654\n",
      "Epoch 36/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3122 - mae: 0.3643\n",
      "Epoch 37/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3129 - mae: 0.3652\n",
      "Epoch 38/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3102 - mae: 0.3651\n",
      "Epoch 39/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3119 - mae: 0.3644\n",
      "Epoch 40/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3124 - mae: 0.3635\n",
      "Epoch 41/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3100 - mae: 0.3638\n",
      "Epoch 42/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3106 - mae: 0.3628\n",
      "Epoch 43/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3108 - mae: 0.3627\n",
      "Epoch 44/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3068 - mae: 0.3638\n",
      "Epoch 45/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3076 - mae: 0.3621\n",
      "Epoch 46/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3087 - mae: 0.3632\n",
      "Epoch 47/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3080 - mae: 0.3617\n",
      "Epoch 48/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3077 - mae: 0.3622\n",
      "Epoch 49/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3059 - mae: 0.3615\n",
      "Epoch 50/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3075 - mae: 0.3622\n",
      "Epoch 51/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3052 - mae: 0.3616\n",
      "Epoch 52/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3061 - mae: 0.3620\n",
      "Epoch 53/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3054 - mae: 0.3613\n",
      "Epoch 54/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3067 - mae: 0.3611\n",
      "Epoch 55/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3023 - mae: 0.3605\n",
      "Epoch 56/150\n",
      "541/541 [==============================] - 1s 1ms/step - loss: 0.3052 - mae: 0.3603\n",
      "Epoch 57/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3030 - mae: 0.3614\n",
      "Epoch 58/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3032 - mae: 0.3611\n",
      "Epoch 59/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3012 - mae: 0.3602\n",
      "Epoch 60/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3026 - mae: 0.3611\n",
      "Epoch 61/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3028 - mae: 0.3606\n",
      "Epoch 62/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3013 - mae: 0.3607\n",
      "Epoch 63/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2994 - mae: 0.3595\n",
      "Epoch 64/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3001 - mae: 0.3608\n",
      "Epoch 65/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3012 - mae: 0.3600\n",
      "Epoch 66/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2989 - mae: 0.3606\n",
      "Epoch 67/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2993 - mae: 0.3596\n",
      "Epoch 68/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2994 - mae: 0.3600\n",
      "Epoch 69/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2993 - mae: 0.3593\n",
      "Epoch 70/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2990 - mae: 0.3605\n",
      "Epoch 71/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.3001 - mae: 0.3602\n",
      "Epoch 72/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2983 - mae: 0.3594\n",
      "Epoch 73/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2973 - mae: 0.3582\n",
      "Epoch 74/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2976 - mae: 0.3596\n",
      "Epoch 75/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2988 - mae: 0.3589\n",
      "Epoch 76/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2983 - mae: 0.3595\n",
      "Epoch 77/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2943 - mae: 0.3587\n",
      "Epoch 78/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2961 - mae: 0.3598\n",
      "Epoch 79/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2992 - mae: 0.3593\n",
      "Epoch 80/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2946 - mae: 0.3580\n",
      "Epoch 81/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2959 - mae: 0.3582\n",
      "Epoch 82/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2910 - mae: 0.3580\n",
      "Epoch 83/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2939 - mae: 0.3587\n",
      "Epoch 84/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2927 - mae: 0.3589\n",
      "Epoch 85/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2929 - mae: 0.3582\n",
      "Epoch 86/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2920 - mae: 0.3585\n",
      "Epoch 87/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2926 - mae: 0.3584\n",
      "Epoch 88/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2923 - mae: 0.3582\n",
      "Epoch 89/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2913 - mae: 0.3575\n",
      "Epoch 90/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2919 - mae: 0.3579\n",
      "Epoch 91/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2913 - mae: 0.3578\n",
      "Epoch 92/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2923 - mae: 0.3582\n",
      "Epoch 93/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2901 - mae: 0.3574\n",
      "Epoch 94/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2907 - mae: 0.3568\n",
      "Epoch 95/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2912 - mae: 0.3579\n",
      "Epoch 96/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2915 - mae: 0.3573\n",
      "Epoch 97/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2885 - mae: 0.3570\n",
      "Epoch 98/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2896 - mae: 0.3557\n",
      "Epoch 99/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2902 - mae: 0.3570\n",
      "Epoch 100/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2886 - mae: 0.3560\n",
      "Epoch 101/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2873 - mae: 0.3556\n",
      "Epoch 102/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2870 - mae: 0.3555\n",
      "Epoch 103/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2877 - mae: 0.3562\n",
      "Epoch 104/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2866 - mae: 0.3563\n",
      "Epoch 105/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2889 - mae: 0.3555\n",
      "Epoch 106/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2891 - mae: 0.3572\n",
      "Epoch 107/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2861 - mae: 0.3561\n",
      "Epoch 108/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2889 - mae: 0.3564\n",
      "Epoch 109/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2841 - mae: 0.3553\n",
      "Epoch 110/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2846 - mae: 0.3562\n",
      "Epoch 111/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2840 - mae: 0.3557\n",
      "Epoch 112/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2850 - mae: 0.3556\n",
      "Epoch 113/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2846 - mae: 0.3554\n",
      "Epoch 114/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2840 - mae: 0.3562\n",
      "Epoch 115/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2827 - mae: 0.3548\n",
      "Epoch 116/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2846 - mae: 0.3564\n",
      "Epoch 117/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2838 - mae: 0.3553\n",
      "Epoch 118/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2825 - mae: 0.3550\n",
      "Epoch 119/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2821 - mae: 0.3552\n",
      "Epoch 120/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2832 - mae: 0.3545\n",
      "Epoch 121/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2805 - mae: 0.3547\n",
      "Epoch 122/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2822 - mae: 0.3551\n",
      "Epoch 123/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2835 - mae: 0.3547\n",
      "Epoch 124/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2834 - mae: 0.3545\n",
      "Epoch 125/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2831 - mae: 0.3547\n",
      "Epoch 126/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2823 - mae: 0.3550\n",
      "Epoch 127/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2827 - mae: 0.3543\n",
      "Epoch 128/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2788 - mae: 0.3542\n",
      "Epoch 129/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2801 - mae: 0.3546\n",
      "Epoch 130/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2811 - mae: 0.3543\n",
      "Epoch 131/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2792 - mae: 0.3546\n",
      "Epoch 132/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2801 - mae: 0.3528\n",
      "Epoch 133/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2809 - mae: 0.3550\n",
      "Epoch 134/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2797 - mae: 0.3538\n",
      "Epoch 135/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2827 - mae: 0.3550\n",
      "Epoch 136/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2800 - mae: 0.3544\n",
      "Epoch 137/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2801 - mae: 0.3538\n",
      "Epoch 138/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2799 - mae: 0.3543\n",
      "Epoch 139/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2788 - mae: 0.3546\n",
      "Epoch 140/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2792 - mae: 0.3532\n",
      "Epoch 141/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2765 - mae: 0.3533\n",
      "Epoch 142/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2780 - mae: 0.3528\n",
      "Epoch 143/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2794 - mae: 0.3536\n",
      "Epoch 144/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2789 - mae: 0.3528\n",
      "Epoch 145/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2783 - mae: 0.3536\n",
      "Epoch 146/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2769 - mae: 0.3523\n",
      "Epoch 147/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2780 - mae: 0.3533\n",
      "Epoch 148/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2798 - mae: 0.3536\n",
      "Epoch 149/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2768 - mae: 0.3534\n",
      "Epoch 150/150\n",
      "541/541 [==============================] - 1s 2ms/step - loss: 0.2799 - mae: 0.3536\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3514 - mae: 0.3850\n",
      "Neural Network 1:\n",
      "                  Loss:  0.5727564692497253\n",
      "   Mean Absolute Error:  0.4735826551914215\n",
      "\n",
      "\n",
      "\n",
      "Neural Network 2:\n",
      "                  Loss:  0.4944940507411957\n",
      "   Mean Absolute Error:  0.46770864725112915\n",
      "\n",
      "\n",
      "\n",
      "Neural Network 3:\n",
      "                  Loss:  0.3513614535331726\n",
      "   Mean Absolute Error:  0.3849944472312927\n"
     ]
    }
   ],
   "source": [
    "# Lets try some combinations out\n",
    "\n",
    "# Neural network one\n",
    "nn1_hist, nn1_loss, nn1_mae=evalNN(\n",
    "                                        model=get_compiled_model(\n",
    "                                                                 inputShape = (7,),\n",
    "                                                                 activationFunction='relu',\n",
    "                                                                 neuronList=[64, 32],\n",
    "                                                                 optimFuction='rmsprop',\n",
    "                                                                 lossFunction='mse',\n",
    "                                                                 metricsFunction='mae'\n",
    "                                                                ),\n",
    "                                        features=df[['bedrooms', 'bathrooms', 'yr_built', 'view', 'waterfront', 'condition', 'sqft_lot']],\n",
    "                                        target=df['price'],\n",
    "                                        epochNum=200\n",
    "                                  )\n",
    "\n",
    "# Neural network two\n",
    "nn2_hist, nn2_loss, nn2_mae=evalNN(\n",
    "                                        model=get_compiled_model(\n",
    "                                                                 inputShape = (7,),\n",
    "                                                                 activationFunction='softmax',\n",
    "                                                                 neuronList=[64, 32, 16],\n",
    "                                                                 optimFuction='adam',\n",
    "                                                                 lossFunction='mse',\n",
    "                                                                 metricsFunction='mae'\n",
    "                                                                ),\n",
    "                                        features=df[['bedrooms', 'bathrooms', 'yr_built', 'view', 'waterfront', 'condition', 'sqft_lot']],\n",
    "                                        target=df['price'],\n",
    "                                        epochNum=100\n",
    "                                  )\n",
    "\n",
    "# Neural network three\n",
    "nn3_hist, nn3_loss, nn3_mae=evalNN(\n",
    "                                        model=get_compiled_model(\n",
    "                                                                 inputShape = (9,),\n",
    "                                                                 activationFunction='sigmoid',\n",
    "                                                                 neuronList=[64, 32, 32],\n",
    "                                                                 optimFuction='rmsprop',\n",
    "                                                                 lossFunction='mse',\n",
    "                                                                 metricsFunction='mae'\n",
    "                                                                ),\n",
    "                                        features=df[['bedrooms', 'bathrooms', 'yr_built', 'view', 'waterfront', 'condition', 'sqft_lot', 'floors', 'grade']],\n",
    "                                        target=df['price'],\n",
    "                                        epochNum=150\n",
    "                                  )\n",
    "\n",
    "print(\"Neural Network 1:\")\n",
    "print(\"                  Loss: \", nn1_loss)\n",
    "print(\"   Mean Absolute Error: \", nn1_mae)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Neural Network 2:\")\n",
    "print(\"                  Loss: \", nn2_loss)\n",
    "print(\"   Mean Absolute Error: \", nn2_mae)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Neural Network 3:\")\n",
    "print(\"                  Loss: \", nn3_loss)\n",
    "print(\"   Mean Absolute Error: \", nn3_mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I copied the end of the output file here, so you can see what I got:\n",
    "(Instead of scrolling down the whole output text file for all the neural networks)\n",
    "\n",
    "     Neural Network 1:\n",
    "                       Loss:  0.5727564692497253\n",
    "        Mean Absolute Error:  0.4735826551914215\n",
    "     \n",
    "     \n",
    "     \n",
    "     Neural Network 2:\n",
    "                       Loss:  0.4944940507411957\n",
    "        Mean Absolute Error:  0.46770864725112915\n",
    "     \n",
    "     \n",
    "     \n",
    "     Neural Network 3:\n",
    "                       Loss:  0.3513614535331726\n",
    "        Mean Absolute Error:  0.3849944472312927\n",
    "\n",
    "# The lowest MAE out of the three combinations I tried was:\n",
    "# Mean Absolute Error:  0.3849944472312927\n",
    "# With Neural Network combination 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
